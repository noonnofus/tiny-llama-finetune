# Tiny-Llama ëª¨ë¸ LoRA ë°©ì‹ ê¸°ë°˜ìœ¼ë¡œ fine-tuning

## ğŸ“œ Description
ì´ ë¦¬í¬ì§€í† ë¦¬ëŠ” **LoRA(Low-Rank Adaptation)** ë°©ì‹ì„ í™œìš©í•˜ì—¬ TinyLlama-1.1B-Chat ëª¨ë¸ì„ fine-tuningí•œ í”„ë¡œì íŠ¸ì…ë‹ˆë‹¤.
ëª¨ë“  í•™ìŠµ ê³¼ì •ì€ **Google Colab** í™˜ê²½ì—ì„œ ìˆ˜í–‰ë˜ì—ˆìœ¼ë©°, í•™ìŠµì— ì‚¬ìš©ëœ ë°ì´í„°ì…‹ì€ [stanford_alpaca](https://github.com/tatsu-lab/stanford_alpaca/tree/main/assets) ë°ì´í„°ì…‹ì…ë‹ˆë‹¤.

## ğŸ§  Fine-tuning
### 1. Hugging Face ë¡œê·¸ì¸ ë° ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°
Hugging Faceì— ë¡œê·¸ì¸í•œ í›„ TinyLlama ëª¨ë¸ì„ ë¶ˆëŸ¬ì˜¤ëŠ” ì½”ë“œì…ë‹ˆë‹¤.
ë¡œê·¸ì¸ ì‹œì—ëŠ” hf_ë¡œ ì‹œì‘í•˜ëŠ” **Access Token**ì´ í•„ìš”í•˜ë©°, í•´ë‹¹ tokenì— ìµœì†Œí•œ **"Read"** ê¶Œí•œì´ ìˆì–´ì•¼ ëª¨ë¸ì„ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
```python3
login("token")

model_id = "TinyLlama/TinyLlama-1.1B-Chat-v1.0" # ì›í•˜ëŠ” ëª¨ë¸

tokenizer = AutoTokenizer.from_pretrained(model_id)
model = AutoModelForCausalLM.from_pretrained(
    model_id,
    load_in_8bit=True,
    device_map="auto",
)
```
### 2. Alpaca ë°ì´í„°ì…‹ ë¶ˆëŸ¬ì˜¤ê¸° ë° í•™ìŠµìš© í”„ë¡¬í”„íŠ¸ í˜•ì‹ìœ¼ë¡œ ë³€í™˜

Stanford Alpaca JSON ë°ì´í„°ë¥¼ Hugging Faceì˜ `datasets.Dataset` í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.  
ì´ë•Œ ëª¨ë¸ í•™ìŠµì— ì í•©í•˜ë„ë¡ **`Instruction`, `Input`, `Response`** êµ¬ì¡°ë¡œ ê°€ê³µí•©ë‹ˆë‹¤.

**Before:**
```json
{
  "instruction": "Create a classification algorithm for a given dataset.", 
  "input": "Dataset of medical images", 
  "output": "We can use a convolutional neural network (CNN) for classifying medical images. ..."
}
```
**After:**
```
### Instruction:
Create a classification algorithm for a given dataset.

### Input:
Dataset of medical images

### Response:
We can use a convolutional neural network (CNN) for classifying medical images. ...
```
***Code:***
```python3
with open("dataset/alpaca_data.json", "r") as f:
    data = json.load(f)

dataset = Dataset.from_list(data)

def generate_prompt(ex):
  if ex["input"]:
    return f"### Instruction:\n{ex['instruction']}\n\n### Input:\n{ex['input']}\n\n### Response:\n{ex['output']}"
  else:
    return f"### Instruction:\n{ex['instruction']}\n\n### Response:\n{ex['output']}"

dataset = dataset.map(lambda x: {"prompt": generate_prompt(x)})

tokenizer = AutoTokenizer.from_pretrained(model_id)
model = AutoModelForCausalLM.from_pretrained(
    model_id,
    torch_dtype=torch.float16
)
```
### 3. LoRA ì„¤ì • ë° ëª¨ë¸ ì ìš©
**LoRA ë€?** <br/>
ê¸°ì¡´ ëª¨ë¸ í•™ìŠµì€ `y = Wx + b` ê°™ì€ ê³„ì‚°ì„ ë°˜ë³µí•´ì„œ **W** ì™€ **b**ì˜ ê°’ì„ ì¡°ì •í•˜ë©° í•™ìŠµíˆì§€ë§Œ ì´ëŸ° í•™ìŠµì€ ìˆ˜ì–µê°œ ì—ì„œ ìˆ˜ë°±ì–µ ê°œì˜ íŒŒë¼ë¯¸í„°ë¥¼ ëª¨ë‘ í•™ìŠµí•´ì•¼ í•˜ê¸° ë•Œë¬¸ì—, í•™ìŠµì‹œê°„ë„ ì˜¤ë˜ ê±¸ë¦¬ê³  ì €ì¥ ê³µê°„ë„ ë§ì´ ì°¨ì§€í•œë‹¤.<br/>
ì´ì— ë°˜í•´ **LoRA ë°©ì‹**ì€ íŒŒë¼ë¯¸í„°, **W**ì™€ **b**ë¥¼ **ê³ ì •** ì‹œí‚¤ê³  **ì‘ì€ í–‰ë ¬**(A, B)ì„ ì‚¬ì´ì— **ì¶”ê°€**ë¡œ ë„£ê³  ê·¸ê²ƒë§Œ í•™ìŠµí•˜ëŠ” ë°©ì‹ì´ê¸° ë•Œë¬¸ì— ì†ë„ì™€ ì €ì¥ ê³µê°„ì´ í¬ê²Œ ì ˆì•½ëœë‹¤.

```python3
# LoRA ì„¤ì •
lora_config = LoraConfig(
    r=8, # í•™ìŠµ íŒŒë¼ë¯¸í„° ê°œìˆ˜
    lora_alpha=16, 
    target_modules=["q_proj", "v_proj"], # íŠ¸ëœìŠ¤í¬ë¨¸ êµ¬ì¡° ì•ˆì— ì–´ë–¤ ëª¨ë“ˆì— LoRAë¥¼ ë¶™ì¼ì§€ ê²°ì •
    lora_dropout=0.05,
    bias="none",
    task_type=TaskType.CAUSAL_LM, # ì–¸ì–´ ëª¨ë¸ë§ ëª…ì‹œ
)

# LoRA ì„¤ì •ì„ Tiny_Llama ëª¨ë¸ì— ì ìš©
model = get_peft_model(model, lora_config)
```
### 4. ë°ì´í„°ì…‹ í† í°í™”
```python3
# ë°ì´í„°ì…‹ì„ í† í°í™”(ì»´í“¨í„°ê°€ í•´ë‹¹ ë°ì´í„°ì…‹ì„ í•™ìŠµí•˜ê¸° ìœ„í•´ì„œ ìˆ«ì IDë¡œ ë³€í™˜)
def tokenize_function(example):
    result = tokenizer(
        example["prompt"], # í”„ë¡¬í”„íŠ¸ì— ê´€í•´ í† í°í™”
        truncation=True, # ìµœëŒ€ ê¸¸ì´ ì´ˆê³¼ì‹œ ì˜ë¼ëƒ„
        padding="max_length",
        max_length=512,
    )
    result["labels"] = result["input_ids"].copy()
    return result

# ì „ì²´ ë°ì´í„°ì…‹ì„ í† í°í™”
tokenized_dataset = dataset.map(tokenize_function, batched=True)
```
### 5. Trainer êµ¬ì„± ë° í•™ìŠµ ì‹œì‘
```python3
# í•™ìŠµ íŒŒë¼ë¯¸í„° ì •ì˜
training_args = TrainingArguments(
    output_dir="./tinyllama-lora", # ê²°ê³¼ ì €ì¥ í´ë” ìœ„ì¹˜
    per_device_train_batch_size=4,
    num_train_epochs=1, # í•™ìŠµ íšŸìˆ˜
    learning_rate=2e-4, # í•™ìŠµë¥ 
    logging_steps=10, # ë¡œê·¸ ì¶œë ¥ ë¹ˆë„
    save_strategy="epoch",
    fp16=True,
    report_to="none",
)

data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)

# í•™ìŠµ ê°œì²´ ì„¤ì •
trainer = Trainer(
    model=model, # í•™ìŠµí•  ëª¨ë¸
    args=training_args, # í•™ìŠµ ì„¤ì •
    train_dataset=tokenized_dataset, # ë°ì´í„°ì…‹
    data_collator=data_collator, 
)

# í•™ìŠµ ì‹œì‘
trainer.train()
```

## ğŸ“¦ Installation 
```bash
# í•´ë‹¹ ë¦¬í¬ì§€í† ë¦¬ í´ë¡ í›„ ì´ë™
git clone https://github.com/noonnofus/tiny-llama-finetune.git && cd tiny-llama-finetune

# ì˜ì¡´ì„± ì„¤ì¹˜
pip install -r requirements.txt

# í•™ìŠµ ì‹œì‘
python3 script.py
```
