# Tiny-Llama ëª¨ë¸ LoRA ë°©ì‹ ê¸°ë°˜ìœ¼ë¡œ fine-tuning

## ğŸ“œ Description
ì´ ë¦¬í¬ì§€í† ë¦¬ëŠ” LoRA(Low-Rank Adaptation) ë°©ì‹ì„ í™œìš©í•˜ì—¬ TinyLlama-1.1B-Chat ëª¨ë¸ì„ fine-tuningí•œ í”„ë¡œì íŠ¸ì…ë‹ˆë‹¤.
ëª¨ë“  í•™ìŠµ ê³¼ì •ì€ Google Colab í™˜ê²½ì—ì„œ ìˆ˜í–‰ë˜ì—ˆìœ¼ë©°, í•™ìŠµì— ì‚¬ìš©ëœ ë°ì´í„°ì…‹ì€ [stanford_alpaca](https://github.com/tatsu-lab/stanford_alpaca/tree/main/assets) ë°ì´í„°ì…‹ì…ë‹ˆë‹¤.

## ğŸ§  Fine-tuning
### 1. Hugging Face ë¡œê·¸ì¸ ë° ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°
Hugging Faceì— ë¡œê·¸ì¸í•œ í›„ TinyLlama ëª¨ë¸ì„ ë¶ˆëŸ¬ì˜¤ëŠ” ì½”ë“œì…ë‹ˆë‹¤.
ë¡œê·¸ì¸ ì‹œì—ëŠ” hf_ë¡œ ì‹œì‘í•˜ëŠ” í† í°ì´ í•„ìš”í•˜ë©°, í•´ë‹¹ tokenì— ìµœì†Œí•œ "Read" ê¶Œí•œì´ ìˆì–´ì•¼ ëª¨ë¸ì„ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
```python3
login("token")

model_id = "TinyLlama/TinyLlama-1.1B-Chat-v1.0" # ì›í•˜ëŠ” ëª¨ë¸

tokenizer = AutoTokenizer.from_pretrained(model_id)
model = AutoModelForCausalLM.from_pretrained(
    model_id,
    load_in_8bit=True,
    device_map="auto",
)
```
### 2. Alpaca ë°ì´í„°ì…‹ ë¶ˆëŸ¬ì˜¤ê¸° ë° í•™ìŠµìš© í”„ë¡¬í”„íŠ¸ í˜•ì‹ìœ¼ë¡œ ë³€í™˜

Stanford Alpaca JSON ë°ì´í„°ë¥¼ Hugging Faceì˜ `datasets.Dataset` í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.  
ì´ë•Œ ëª¨ë¸ í•™ìŠµì— ì í•©í•˜ë„ë¡ `Instruction`, `Input`, `Response` êµ¬ì¡°ë¡œ ê°€ê³µí•©ë‹ˆë‹¤.

**Before:**
```json
{
  "instruction": "Create a classification algorithm for a given dataset.", 
  "input": "Dataset of medical images", 
  "output": "We can use a convolutional neural network (CNN) for classifying medical images. ..."
}
```
**After:**
```
### Instruction:
Create a classification algorithm for a given dataset.

### Input:
Dataset of medical images

### Response:
We can use a convolutional neural network (CNN) for classifying medical images. ...
```
***Code:***
```python3
with open("dataset/alpaca_data.json", "r") as f:
    data = json.load(f)

dataset = Dataset.from_list(data)

def generate_prompt(ex):
  if ex["input"]:
    return f"### Instruction:\n{ex['instruction']}\n\n### Input:\n{ex['input']}\n\n### Response:\n{ex['output']}"
  else:
    return f"### Instruction:\n{ex['instruction']}\n\n### Response:\n{ex['output']}"

dataset = dataset.map(lambda x: {"prompt": generate_prompt(x)})

model_id = "TinyLlama/TinyLlama-1.1B-Chat-v1.0"

tokenizer = AutoTokenizer.from_pretrained(model_id)
model = AutoModelForCausalLM.from_pretrained(
    model_id,
    torch_dtype=torch.float16
)
```
### 3. LoRA ì„¤ì • ë° ëª¨ë¸ ì ìš©
### 4. ë°ì´í„°ì…‹ í† í°í™”
### 5. TrainingArguments ì„¤ì •
### 6. Trainer êµ¬ì„± ë° í•™ìŠµ ì‹œì‘

## ğŸ“¦ Installation 
```bash
# í•´ë‹¹ ë¦¬í¬ì§€í† ë¦¬ í´ë¡ í›„ ì´ë™
git clone https://github.com/noonnofus/tiny-llama-finetune.git && cd tiny-llama-finetune

# ì˜ì¡´ì„± ì„¤ì¹˜
pip install -r requirements.txt

# í•™ìŠµ ì‹œì‘
python3 script.py
```
